## Change Request (CRQ)

**CRQ ID:** CRQ-20250820-002
**Title:** Integrate `hf-dataset-validator-rust` for Codebase Analysis and Dataset Export
**Date:** 2025-08-20
**Requested By:** Gemini CLI (on behalf of User)
**Status:** Proposed

### 1. Justification (ISO 9000, ITIL, GMP, Six Sigma)

This CRQ aims to integrate the `hf-dataset-validator-rust` tool into our development workflow to enhance code quality, improve data management, and streamline our processes.

*   **ISO 9000 (Quality Management):** By systematically analyzing our codebase with `hf-dataset-validator-rust`, we can ensure consistent code quality, identify deviations from standards, and improve the reliability of our software artifacts. Documenting the process ensures traceability.
*   **ITIL (IT Service Management):** Integrating this tool will improve our service transition by providing automated validation of code changes, reducing potential incidents related to new deployments. It supports problem management by offering deeper insights into code structure and potential issues.
*   **GMP (Good Manufacturing Practices):** The tool will enable us to validate our codebase against predefined metrics, ensuring reproducibility and consistency of our software "product." This provides an auditable trail of code quality.
*   **Six Sigma (Process Improvement):** Automating code analysis and data export reduces variability in our development process, minimizes defects by identifying patterns, and provides data-driven insights for continuous improvement.

### 2. Impact (C4 Model)

This change primarily impacts the "Code" and "Component" levels of our C4 model, with implications for "Container" and "Context" as data flows are established.

*   **Code Level:** Introduction of new Rust code for integration and execution of the `hf-dataset-validator-rust` tool.
*   **Component Level:** The `hf-dataset-validator-rust` tool will function as a new component, interacting with existing code analysis components and data storage components.
*   **Container Level:** Data pipelines for exporting to Parquet and Hugging Face will be established or enhanced.
*   **Context Level:** The overall development workflow will incorporate a new automated analysis step.

### 3. Proposed Solution/Plan (UML, Rust-Specific Guidelines)

This plan outlines the steps to integrate and leverage the `hf-dataset-validator-rust` tool.

#### Objective 1: Use `hf-dataset-validator-rust` on our codebase.

*   **Action:** Execute the `hf-dataset-validator-rust` binary against the `libminizinc` codebase.
*   **Rust-Specific:** Ensure the tool is built and accessible within the project environment.
*   **UML (Activity Diagram - conceptual):**
    *   Start
    *   Build `hf-dataset-validator-rust`
    *   Execute `hf-dataset-validator-rust` with codebase path
    *   End

#### Objective 2: Learn from it to export our data to Parquet.

*   **Action:** Analyze the output formats and capabilities of `hf-dataset-validator-rust` to understand how it generates Parquet data. Adapt our existing data export mechanisms (e.g., in `doc_to_minizinc_data`) or integrate directly with the validator's output.
*   **Rust-Specific:** Study the `hf-dataset-validator-rust` source code (if necessary) to understand its Parquet writing logic.
*   **UML (Class Diagram - conceptual):** Identify key structs/traits in `hf-dataset-validator-rust` related to data output (e.g., `DatasetWriter`, `ParquetConfig`).

#### Objective 3: Show that there is an equivalence between the two datasets produced.

*   **Action:** Compare the Parquet datasets produced by `hf-dataset-validator-rust` with datasets generated by our existing `doc_to_minizinc_data` tool (or a subset thereof). This involves schema comparison, data validation, and potentially statistical analysis of embeddings.
*   **Rust-Specific:** Develop a Rust utility (e.g., a new subcommand in `doc_to_minizinc_data` or a new crate) to perform this comparison. Leverage `arrow` and `parquet` crates for reading and comparing data.
*   **UML (Sequence Diagram - conceptual for comparison):**
    *   User -> ComparisonTool: `compare_datasets(dataset1_path, dataset2_path)`
    *   ComparisonTool -> ParquetReader: `read_schema(path)`
    *   ParquetReader --> ComparisonTool: `schema`
    *   ComparisonTool -> ParquetReader: `read_data(path)`
    *   ParquetReader --> ComparisonTool: `data_batches`
    *   ComparisonTool: Perform schema and data comparison
    *   ComparisonTool --> User: `report_equivalence(result)`

#### Objective 4: Merge the codebases together.

*   **Action:** Integrate the relevant parts of `hf-dataset-validator-rust` into our main `libminizinc` project, or establish clear interfaces for its use as a standalone tool. This will involve careful module organization and dependency management.
*   **Rust-Specific:** Adhere to the "one declaration per file" principle. Use `[patch.crates-io]` or vendoring as appropriate.
*   **C4 Model (Component Diagram - updated):** Show `hf-dataset-validator-rust` as a first-class component within the `libminizinc` system.

### 4. Verification (ISO 9000, Six Sigma)

*   **Unit Tests:** Develop comprehensive unit tests for all new code, ensuring high code coverage (Six Sigma - minimize defects).
*   **Integration Tests:** Create integration tests to verify the end-to-end data flow from codebase analysis to dataset export and comparison.
*   **Performance Benchmarks:** (Six Sigma - data-driven decision-making) Measure the performance impact of the new tool and data generation processes.
*   **Documentation Review:** Ensure all new code and processes are documented according to ISO 9000 standards.

### 5. Rollback Plan (ITIL)

In case of issues, the changes can be reverted by:

1.  **Git Revert:** Reverting the specific commit(s) that introduced this feature.
2.  **Manual Cleanup:** Removing any generated files or directories created by the tool.

### 6. Approvals

*   **Development Lead:** [Signature/Date]
*   **QA Lead:** [Signature/Date]
*   **Product Owner:** [Signature/Date]
