The "onboarding and training" of LLM agents on the codebase and documentation within this project refers primarily to **sophisticated prompt engineering and iterative feedback mechanisms** rather than traditional retraining of the large language models themselves. The goal is to enable LLMs to effectively understand and interact with specialized domains like MiniZinc and combinatorial optimization.

### Leveraging Existing LLMs with Contextual Information

*   The project aims to utilize pre-trained LLMs, such as GPT models or LLaMa, by providing them with **specific instructions and contextual information** rather than directly re-training them on the entire codebase.
*   This approach is critical because LLMs often struggle with niche or domain-specific languages (DSLs) like MiniZinc if they haven't been extensively trained on them, leading to syntactical errors and a decline in code quality.

### Prompt Engineering Strategies

*   **Role Definition (System Prompts)**: LLM agents are "onboarded" by defining their precise role, for example, as an **"expert MiniZinc developer"** or a "programming assistant," within the system prompt. This guides the LLM on the type of output and focus required for the task.
*   **Contextual Data Provision**: Prompts are augmented with crucial details to give the LLM a deeper understanding of the problem and expected output:
    *   **Input Data Nomenclature**: Providing explanations of parameters, their meaning, format, and corresponding symbols in `.dzn` files.
    *   **Concrete Usage Examples**: Including few-shot examples from `.dzn` files to guide the LLM's behavior and introduce domain-specific knowledge.
    *   **Parameter Dimensionality (Shape Information)**: Augmenting prompts with details about the shape of parameters, such as whether they are scalar or n-dimensional lists.
    *   **Knowledge Graph Information**: Integrating intermediate knowledge representations in the form of knowledge graphs into the prompts to provide structured information about parameters, variables, constraints, and objectives. This is generated by the LLM itself based on problem descriptions and input data nomenclature.
    *   **Output Specification**: Clearly explaining the expected output format, including the names and shapes of decision variables, especially for satisfaction problems.
*   **Chain-of-Thought (CoT) Prompting**: LLMs are instructed to show their **reasoning steps and thought processes**. This technique helps them tackle complex, multi-step problems and improves their ability to generate accurate solutions by allocating more computational effort to intermediate reasoning stages.
*   **Decomposition-based Prompting**: The modeling task can be split into sequential sub-tasks, with the LLM generating components that are then combined. This structured approach helps manage complexity and improves accuracy.
*   **Grammar Prompting**: This ensures the LLM adheres to specific output formats by providing syntax examples and best practices for the target language (e.g., MiniZinc), acting as "guard rails" that force perfectly structured output for solvers.

### Iterative Refinement and Learning from Development Cycles

*   **Incremental Validation**: After each code fragment is generated by the LLM, the system validates it using MiniZinc methods (or Python's AST module for Python-based backends). If an error occurs (syntax, typing, or semantic), the system returns **precise error diagnostics** to the LLM.
*   **Dynamic Correction**: The LLM agent observes these errors and attempts corrections, often by modifying or replacing parts of the code. This creates a **real-time feedback loop**, enabling continuous refinement of the AI-generated models.
*   **Adaptive Prompts**: Feedback regarding the performance of generated constraints (e.g., errors, unsatisfiability, or solution quality) can be appended to the prompt for subsequent iterations [previous turn]. This allows the LLM to continuously learn and adapt its strategies based on its own development cycles.

### Fine-tuning of Embedding Models for Retrieval Augmented Generation (RAG)

*   Beyond prompt engineering for the LLMs themselves, the project also uses **fine-tuning for embedding models**, particularly for tasks like retrieval. This involves generating data pairs (question and relevant chunk) or triplets (anchor, positive, negative) to train the embedding models to better represent the relevance between queries and documents.
*   This fine-tuning aims to improve retrieval accuracy, especially for niche data sets that LLMs might not inherently "understand well" from their general pre-training.
